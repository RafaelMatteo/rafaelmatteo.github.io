<!DOCTYPE html>
<html lang="es">
  <head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Machine Learning</title>

    <script src="../static/js/scripts.js"></script>
    <script src="../static/js/bootstrap.bundle.min.js"></script>

    <link href="../static/icons/css/all.min.css" rel="stylesheet">
    <link href="../static/icons/css/fontawesome.min.css" rel="stylesheet">
    <link href="../static/css/bootstrap.min.css" rel="stylesheet">

    <link rel="stylesheet" type="text/css" href="../static/css/styles_content.css">
  </head>
  <body>
    <div class="container mt-3 pe-0" style="background-color:orange;">
      <div class="container ps-2 text-light fs-4 fw-bold" style="background-color: mediumblue;">
        Machine Learning...
      </div>
    </div>

    <div class="container mt-2">
      <div class="row">
        <div class="col">
          <div class="card-body p-0">

            <div class="container mt-2">
              <div class="row">
                <!-- Columna para la imagen -->
                <div class="col-md-4">
                  <img src="../static/images/data_science.jpg" class="rounded float-end mb-3 w-100" alt="Machine Learning">
                </div>
                <!-- Columna para el texto -->
                <div class="col-md-8">
                  <p class="card-text">
                    Machine learning is at the heart of many current technological innovations.
                    In this section, I show my mastery of this powerful tool. I have worked with a variety of machine learning 
                    algorithms, from the simplest ones like linear regression to the most complex ones like ensemble models.
                    Using frameworks such as scikit-learn, TensorFlow and XGBoost, I have developed predictive models that 
                    improve the accuracy and efficiency of solutions.
                    <br>
                    Here you will find projects ranging from image classification to numerical value prediction,
                    each optimized through hyperparameter tuning and cross-validation techniques.
                  </p>
                </div>
              </div>
            </div>

            <ul class="mt-2">
              <li>
                <h5><strong>Recipes</strong></h5>
              </li>
              <ul>
                <li>
                  <h6><strong>From modeling to evaluation.</strong></h6>
                  Dataset Description:<br>
                  The data was compiled by a researcher named Yong-Yeol Ahn, who extracted tens of thousands of food 
                  recipes (cuisines and ingredients) from three different websites, namely:
                  <ul>
                    <li><a href="www.allrecipes.com" target="_blank">www.allrecipes.com</a></li>
                    <li><a href="www.epicurious.com" target="_blank">www.epicurious.com</a></li>
                    <li><a href="www.menupan.com" target="_blank">www.menupan.com</a></li>
                  </ul>
                  Our dataset consists of several recipes and their respective ingredients. <br>
                  Each row represents a recipe, and for each recipe, the corresponding geographic area is documented
                  and whether or not the ingredients exist in the recipe, starting with almond
                  and ending with zucchini. <br>
                  We want to determine which ingredients are most commonly used in each geographic area. <br>
                  For this reason, the name of each recipe was removed, leaving only the geographic area and
                  ingredients.
                  <br>
                  <br>
                  <li>
                    Dataset: recipes.csv
                  </li>
                  <li>
                    Tools: Python, Jupyter Lab, Numpy, Pandas, Matplotlib, Graphviz, Scikit-Learn
                  </li>
                  <li>
                    You can download the complete project (zip).
                    <br>
                    <a class="btn_dwld m-2" id="btn_dwld" href="./projects/machine_learning/Recipes.zip" download="Recipes.zip">Recipes.zip</a>
                  </li>
                </li>
              </ul>
            </ul>
              
            <ul>
              <li>
                <h5><strong>Automovile</strong></h5>
              </li>
              <ul>
                <li>
                  <h6><strong>Predicting car prices</strong></h6>
                  This data set consists of three types of entities: (a) a car's specification
                  in terms of various features, (b) its assigned insurance risk rating, (c)
                  its normalized losses in use compared to other cars. <br>
                  The second rating corresponds to the degree to which the car is riskier than its price indicates.
                  Cars are initially assigned a risk factor symbol associated with their price.
                  Then, if it is riskier (or less), this symbol is adjusted by moving it up (or down)
                  on the scale. Actuaries call this process "symbolization." A value of +3 indicates that the
                  car is risky, -3 that it is probably quite safe. <br>
                  The third factor is the relative average loss payment per insured vehicle year.
                  This value is normalized for all cars within a particular size classification (small 2-door, 
                  station wagon, sport/special, etc.) and represents the average loss per car per year. <br>
                  <strong>Note: </strong> <br>
                  After evaluating multiple machine learning models for predicting car prices, XGBoost stood out as 
                  the best option. This model showed superior performance with a Mean Square Error (MSE) of 0.09, a 
                  Mean Absolute Error (MAE) of $200, and a Coefficient of Determination (RÂ²) of 0.95. These results 
                  reflect the high accuracy and fine-tuning ability of the model, making it a reliable tool for 
                  estimating prices in the automotive market. XGBoost not only minimized errors in predictions, but 
                  also explained 95% of the variability in prices, proving to be the optimal solution for this project. <br>
                  <br>
                  <li>
                    Dataset extracted from: <a href="https://archive.ics.uci.edu/dataset/10/automobile" target="_blank">UCI Machine Learning Repository</a>
                  </li>
                  <li>
                    Tools: Python, Jupyter Lab, Numpy, Pandas, Scikit-Learn, StatsModels, DateTime, Scipy, Wget, XGBoost, Matplotlib, Seaborn,
                    GridSearchCV, RandomizedSearchCV, LinearRegression, Ridge, RandomForestRegressor, GradientBoostingRegressor, Pipeline,
                    mean_squared_error, mean_absolute_error, r2_score, RFE, PCA
                  </li>
                  <li>
                    You can download the complete project (zip).
                    <br>
                    <a class="btn_dwld m-2" id="btn_dwld" href="./projects/machine_learning/Automovile_V11.zip" download="Automovile_V11.zip">Automovile_V11.zip</a>
                    <a class="btn_dwld m-2" id="btn_dwld" href="./projects/machine_learning/Automovile_V11.pdf" download="Automovile_V11.pdf">Automovile_V11.pdf</a>
                  </li>
                </li>
              </ul>  
            </ul>    

          </div>
        </div>
      </div>
      <p class="card-text text-end mt-2"><small class="text-muted">Last update: August 12, 2024</small></p>  
    </div>
  </body>
</html>
